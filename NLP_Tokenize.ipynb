{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNhf0WJa1ZlX/NYqzTgf7K4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":81,"metadata":{"id":"4Mr1QeGAQL0R","executionInfo":{"status":"ok","timestamp":1675599090228,"user_tz":-330,"elapsed":621,"user":{"displayName":"Umair Khan","userId":"09383080091704648063"}}},"outputs":[],"source":["from nltk.tokenize import WordPunctTokenizer as punctTok\n","from nltk.tokenize import WhitespaceTokenizer as spaceTok\n","from nltk.tokenize import TreebankWordTokenizer as tbwTok\n","from nltk.tokenize import TweetTokenizer as twtTok\n"]},{"cell_type":"code","source":["Sentence = \"you are going to Enjoy this Sunday, aren't you?\"\n","punctTokens=punctTok().tokenize(Sentence)\n","print(punctTokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iFgpKZCoSBmt","executionInfo":{"status":"ok","timestamp":1675596227465,"user_tz":-330,"elapsed":726,"user":{"displayName":"Umair Khan","userId":"09383080091704648063"}},"outputId":"6ae20139-f25f-4b71-883f-a88aa45975b0"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["['you', 'are', 'going', 'to', 'Enjoy', 'this', 'Sunday', ',', 'aren', \"'\", 't', 'you', '?']\n"]}]},{"cell_type":"code","source":["spcTok = spaceTok().tokenize(Sentence)\n","print(spcTok)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-w_7ThCWUCCG","executionInfo":{"status":"ok","timestamp":1675596246874,"user_tz":-330,"elapsed":4,"user":{"displayName":"Umair Khan","userId":"09383080091704648063"}},"outputId":"d6fcb179-ca03-40ad-ad5c-a0fda53a74a4"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["['you', 'are', 'going', 'to', 'Enjoy', 'this', 'Sunday,', \"aren't\", 'you?']\n"]}]},{"cell_type":"code","source":["tbTok = tbwTok().tokenize(Sentence)\n","print(tbTok)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9VbYHbQzUafO","executionInfo":{"status":"ok","timestamp":1675597129802,"user_tz":-330,"elapsed":5,"user":{"displayName":"Umair Khan","userId":"09383080091704648063"}},"outputId":"577bd13c-f122-4bd1-af7b-92309f212e4a"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["['you', 'are', 'going', 'to', 'Enjoy', 'this', 'Sunday', ',', 'are', \"n't\", 'you', '?']\n"]}]},{"cell_type":"code","source":["tweet= \"@sammy and @co are going for --->vacation, contact: 7666851879, it will be waaay tooooo much <3 funnnn!!!\"\n","twit = twtTok(preserve_case = True, reduce_len=True,strip_handles=True,match_phone_numbers=True).tokenize(tweet)\n","print(twit)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60y5SQrHXyPp","executionInfo":{"status":"ok","timestamp":1675597769211,"user_tz":-330,"elapsed":4,"user":{"displayName":"Umair Khan","userId":"09383080091704648063"}},"outputId":"8bfd4864-de3b-4cc3-d29c-233fbfea5716"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["['and', 'are', 'going', 'for', '--->', 'vacation', ',', 'contact', ':', '7666851879', ',', 'it', 'will', 'be', 'waaay', 'tooo', 'much', '<3', 'funnn', '!', '!', '!']\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import MWETokenizer\n","mweTok = MWETokenizer([('I','am'),('as','soon','as')])\n","mweTok.add_mwe(('you','should'))\n","strin =\"I am an Engineer, as soon as you see defect you should report\"\n","stk= mweTok.tokenize(strin.split())\n","print(stk)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-aL6XjRaUsc","executionInfo":{"status":"ok","timestamp":1675599464329,"user_tz":-330,"elapsed":4,"user":{"displayName":"Umair Khan","userId":"09383080091704648063"}},"outputId":"1189309d-4059-4d1c-f0ad-af7012dda678"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["['I_am', 'an', 'Engineer,', 'as_soon_as', 'you', 'see', 'defect', 'you_should', 'report']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0qRQveFrgr_D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["help(nltk.tokenize.MWETokenizer )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-n30wguHTH_c","executionInfo":{"status":"ok","timestamp":1675598063549,"user_tz":-330,"elapsed":8,"user":{"displayName":"Umair Khan","userId":"09383080091704648063"}},"outputId":"08d3aee2-3720-4cb4-d7b8-d6266df67426"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on class MWETokenizer in module nltk.tokenize.mwe:\n","\n","class MWETokenizer(nltk.tokenize.api.TokenizerI)\n"," |  MWETokenizer(mwes=None, separator='_')\n"," |  \n"," |  A tokenizer that processes tokenized text and merges multi-word expressions\n"," |  into single tokens.\n"," |  \n"," |  Method resolution order:\n"," |      MWETokenizer\n"," |      nltk.tokenize.api.TokenizerI\n"," |      abc.ABC\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, mwes=None, separator='_')\n"," |      Initialize the multi-word tokenizer with a list of expressions and a\n"," |      separator\n"," |      \n"," |      :type mwes: list(list(str))\n"," |      :param mwes: A sequence of multi-word expressions to be merged, where\n"," |          each MWE is a sequence of strings.\n"," |      :type separator: str\n"," |      :param separator: String that should be inserted between words in a multi-word\n"," |          expression token. (Default is '_')\n"," |  \n"," |  add_mwe(self, mwe)\n"," |      Add a multi-word expression to the lexicon (stored as a word trie)\n"," |      \n"," |      We use ``util.Trie`` to represent the trie. Its form is a dict of dicts.\n"," |      The key True marks the end of a valid MWE.\n"," |      \n"," |      :param mwe: The multi-word expression we're adding into the word trie\n"," |      :type mwe: tuple(str) or list(str)\n"," |      \n"," |      :Example:\n"," |      \n"," |      >>> tokenizer = MWETokenizer()\n"," |      >>> tokenizer.add_mwe(('a', 'b'))\n"," |      >>> tokenizer.add_mwe(('a', 'b', 'c'))\n"," |      >>> tokenizer.add_mwe(('a', 'x'))\n"," |      >>> expected = {'a': {'x': {True: None}, 'b': {True: None, 'c': {True: None}}}}\n"," |      >>> tokenizer._mwes == expected\n"," |      True\n"," |  \n"," |  tokenize(self, text)\n"," |      :param text: A list containing tokenized text\n"," |      :type text: list(str)\n"," |      :return: A list of the tokenized text with multi-words merged together\n"," |      :rtype: list(str)\n"," |      \n"," |      :Example:\n"," |      \n"," |      >>> tokenizer = MWETokenizer([('hors', \"d'oeuvre\")], separator='+')\n"," |      >>> tokenizer.tokenize(\"An hors d'oeuvre tonight, sir?\".split())\n"," |      ['An', \"hors+d'oeuvre\", 'tonight,', 'sir?']\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __abstractmethods__ = frozenset()\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from nltk.tokenize.api.TokenizerI:\n"," |  \n"," |  span_tokenize(self, s: str) -> Iterator[Tuple[int, int]]\n"," |      Identify the tokens using integer offsets ``(start_i, end_i)``,\n"," |      where ``s[start_i:end_i]`` is the corresponding token.\n"," |      \n"," |      :rtype: Iterator[Tuple[int, int]]\n"," |  \n"," |  span_tokenize_sents(self, strings: List[str]) -> Iterator[List[Tuple[int, int]]]\n"," |      Apply ``self.span_tokenize()`` to each element of ``strings``.  I.e.:\n"," |      \n"," |          return [self.span_tokenize(s) for s in strings]\n"," |      \n"," |      :yield: List[Tuple[int, int]]\n"," |  \n"," |  tokenize_sents(self, strings: List[str]) -> List[List[str]]\n"," |      Apply ``self.tokenize()`` to each element of ``strings``.  I.e.:\n"," |      \n"," |          return [self.tokenize(s) for s in strings]\n"," |      \n"," |      :rtype: List[List[str]]\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from nltk.tokenize.api.TokenizerI:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n","\n"]}]}]}